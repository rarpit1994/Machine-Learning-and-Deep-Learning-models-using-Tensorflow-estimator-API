{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Logging before flag parsing goes to stderr.\n",
      "W1005 00:05:57.547605 13236 deprecation.py:323] From <ipython-input-5-a839aeb82f4b>:1: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "W1005 00:05:57.549597 13236 deprecation.py:323] From C:\\Users\\ARPIT\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "W1005 00:05:57.680093 13236 deprecation.py:323] From C:\\Users\\ARPIT\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "W1005 00:05:59.297505 13236 deprecation.py:323] From C:\\Users\\ARPIT\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1005 00:05:59.954797 13236 deprecation.py:323] From C:\\Users\\ARPIT\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "W1005 00:05:59.963781 13236 deprecation.py:323] From C:\\Users\\ARPIT\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W1005 00:06:00.798560 13236 deprecation.py:323] From C:\\Users\\ARPIT\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32,[None,784])\n",
    "y_ = tf.placeholder(tf.float32,[None,10])\n",
    "\n",
    "#Add Function to create weight variable\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "#Add function to create bias variable\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1,shape = shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "#Add convolution function\n",
    "def conv2d(x, W):\n",
    "    return (tf.nn.conv2d(x, W, strides=[1, 1, 1, 1], padding = \"SAME\"))\n",
    "\n",
    "#Add pooling function\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize = [1, 2, 2, 1], strides = [1, 2, 2, 1], padding= \"SAME\")\n",
    "\n",
    "#Reshape the data for inputting it to the input layer\n",
    "x_image= tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "#create 1st convolutional and pooling layer\n",
    "W_conv1 = weight_variable([5, 5, 1, 32])\n",
    "b_conv1 = bias_variable([32])\n",
    "\n",
    "h_conv1 = tf.nn.relu(conv2d(x_image,  W_conv1) + b_conv1)\n",
    "h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "#create 2nd convolutional and pooling layer\n",
    "W_conv2 = weight_variable([5, 5, 32, 64])\n",
    "b_conv2 = bias_variable([64])\n",
    "\n",
    "h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "#create 3rd convolutional and pooling layer\n",
    "W_conv3 = weight_variable([5, 5, 64, 128])\n",
    "b_conv3 = bias_variable([128])\n",
    "\n",
    "h_conv3 = tf.nn.relu(conv2d(h_pool2, W_conv3) + b_conv3)\n",
    "h_pool3 = max_pool_2x2(h_conv3)\n",
    "\n",
    "#1st fully connected layer\n",
    "W_fc1 = weight_variable([128*4*4, 1024])\n",
    "b_fc1 = bias_variable([1024])\n",
    "\n",
    "#Reshape the last convolutional layer output to connect with fully connected layer\n",
    "h_pool2_flat = tf.reshape(h_pool3, [-1, 128*4*4])\n",
    "h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1)+ b_fc1)\n",
    "\n",
    "#Add dropout layer\n",
    "rate = tf.placeholder(tf.float32)\n",
    "h_fc1_drop = tf.nn.dropout(h_fc1, rate)\n",
    "\n",
    "#output layer\n",
    "W_fc2 = weight_variable([1024, 10])\n",
    "b_fc2 = bias_variable([10])\n",
    "\n",
    "y_conv = tf.matmul(h_fc1_drop, W_fc2)+ b_fc2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0, training accuracy 0.08 \n",
      "step 100, training accuracy 0.78 \n",
      "step 200, training accuracy 0.86 \n",
      "step 300, training accuracy 0.94 \n",
      "step 400, training accuracy 0.9 \n",
      "step 500, training accuracy 0.92 \n",
      "step 600, training accuracy 0.9 \n",
      "step 700, training accuracy 0.98 \n",
      "step 800, training accuracy 0.98 \n",
      "step 900, training accuracy 0.98 \n",
      "step 1000, training accuracy 1 \n",
      "test accuracy 0.92\n",
      "test accuracy 0.94\n",
      "test accuracy 0.96\n",
      "test accuracy 0.98\n",
      "test accuracy 0.98\n",
      "test accuracy 0.96\n",
      "test accuracy 0.98\n",
      "test accuracy 0.94\n",
      "test accuracy 0.92\n",
      "test accuracy 1\n",
      "test accuracy 1\n"
     ]
    }
   ],
   "source": [
    "#Network Base of Computation\n",
    "\n",
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_ , logits=y_conv))\n",
    "train_step = tf.train.AdamOptimizer(1e-4).minimize(cross_entropy)\n",
    "\n",
    "#Prediction Measure\n",
    "correct_prediction = tf.equal(tf.argmax(y_conv , 1),tf.arg_max(y_ , 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction , tf.float32))\n",
    "\n",
    "#Mention about the number of epochs training has to happen\n",
    "epochs= 1100\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    for i in range(epochs):\n",
    "        #Read a batch of training data\n",
    "        batch = mnist.train.next_batch(50)\n",
    "        if i %100 == 0:\n",
    "            #Add the training accuracy step\n",
    "            train_accuracy = accuracy.eval(feed_dict = {x: batch[0] , y_:batch[1], rate:1.0})\n",
    "            print(\"step %d, training accuracy %g \" % (i,train_accuracy))\n",
    "        train_step.run(feed_dict = {x:batch[0], y_:batch[1], rate:0.5})\n",
    "    for i in range(epochs):\n",
    "        #Read a batch of testing data\n",
    "        batch = mnist.test.next_batch(50)\n",
    "        if i % 100 == 0:\n",
    "            #Add the testing accuracy data\n",
    "            test_accuracy = accuracy.eval(feed_dict = {x: batch[0], y_:batch[1], rate:1.0})\n",
    "            print('test accuracy %g'% test_accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
